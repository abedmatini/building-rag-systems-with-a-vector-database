{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Module 3 Assignment - Building RAG systems with a Vector Database\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment you will work with [Weaviate API](https://weaviate.io/) to build a tiny RAG system. You will:\n",
    "\n",
    "- Load a [collection](https://weaviate.io/developers/weaviate/manage-data/collections) for BBC News data.\n",
    "- Use the Weaviate API to retrieve documents from the vector database.\n",
    "- Create functions to retrieve data based on Semantic Search, BM25 and Hybrid Search (using RRF) using the Weaviate API.\n",
    "- Use an LLM to generate responses.\n",
    "\n",
    "**IMPORTANT**: This assignment assumes you know how to handle simple tasks with collections in the Weaviate API. If you are not familiar with it yet, please read the Ungraded Lab on the Weaviate API! Furthermore, the data you will be working here is already *chunked*. You can get more hands-on experience on chunking reading the Ungraded Lab on chunking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Loading the libraries](#1)\n",
    "- [ 2 - Setting up the Weaviate Client and loading the data](#2)\n",
    "  - [ 2.1 Loading the Weaviate Client](#2-1)\n",
    "  - [ 2.2 Loading the data](#2-2)\n",
    "- [ 3 - Loading the Collection](#3)\n",
    "  - [ 3.1 Metadata filtering](#3-1)\n",
    "    - [ Exercise 1](#ex01)\n",
    "  - [ 3.2 Semantic search](#3-2)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 3.3 BM25 Serach](#3-3)\n",
    "    - [ Exercise 3](#ex03)\n",
    "  - [ 3.4 Hybrid search](#3-4)\n",
    "    - [ Exercise 4](#ex04)\n",
    "    - [ Exercise 5](#ex05)\n",
    "- [ 4 - Incorporating the Weaviate API into our previous schema](#4)\n",
    "  - [ 4.1 Generating the final prompt](#4-1)\n",
    "  - [ 4.2 LLM call](#4-2)\n",
    "- [ 5 - Experimenting with Your RAG System](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)\n",
    "\n",
    "---\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "- - To submit your notebook for grading, first save it by clicking the 💾 icon on the top left of the page and then click on the <span style=\"background-color: blue; color: white; padding: 3px 5px; font-size: 16px; border-radius: 5px;\">Submit assignment</span> button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Loading the libraries\n",
    "\n",
    "---\n",
    "\n",
    "Run the cell below to load the necessary libraries for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import weaviate\n",
    "from weaviate.classes.query import (\n",
    "    Filter, \n",
    "    Rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'flask_app'\n",
      " * Debug mode: off\n",
      "Weaviate is ready!\n",
      "Connected to Weaviate via Docker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\xampp9\\htdocs\\building-rag-systems-with-a-vector-database\\venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import flask_app\n",
    "import weaviate_server\n",
    "from utils import (\n",
    "    generate_with_single_input,\n",
    "    print_object_properties,\n",
    "    display_widget\n",
    ")\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Setting up the Weaviate Client and loading the data\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you will set up the Weaviate client and load the data, which consists of the [BBC news dataset](https://www.kaggle.com/datasets/gpreda/bbc-news) adapted from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-1'></a>\n",
    "### 2.1 Loading the Weaviate Client\n",
    "\n",
    "Let's connect the Weaviate client to begin working with the Weaviate API. The server is already running on the backend. \n",
    "\n",
    "**Troubleshooting:**\n",
    "\n",
    "- If you encounter issues loading the next cell, try restarting your kernel, clicking in the circled arrow in the panel above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local(port=8080, grpc_port=50051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Collection doesn't exist. Creating and loading data...\n",
      "❌ Error during setup: _Vectorizer.text2vec_transformers() got an unexpected keyword argument 'model'\n",
      "You may need to run: python setup_collection.py\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_Vectorizer.text2vec_transformers() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Create collection with schema\u001b[39;00m\n\u001b[32m     15\u001b[39m collection = client.collections.create(\n\u001b[32m     16\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mbbc_collection\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     properties=[\n\u001b[32m     18\u001b[39m         Property(name=\u001b[33m\"\u001b[39m\u001b[33marticle_content\u001b[39m\u001b[33m\"\u001b[39m, data_type=DataType.TEXT),\n\u001b[32m     19\u001b[39m         Property(name=\u001b[33m\"\u001b[39m\u001b[33mchunk\u001b[39m\u001b[33m\"\u001b[39m, data_type=DataType.TEXT),\n\u001b[32m     20\u001b[39m         Property(name=\u001b[33m\"\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m\"\u001b[39m, data_type=DataType.INT),\n\u001b[32m     21\u001b[39m         Property(name=\u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m, data_type=DataType.TEXT),\n\u001b[32m     22\u001b[39m         Property(name=\u001b[33m\"\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m\"\u001b[39m, data_type=DataType.TEXT),\n\u001b[32m     23\u001b[39m         Property(name=\u001b[33m\"\u001b[39m\u001b[33mpubDate\u001b[39m\u001b[33m\"\u001b[39m, data_type=DataType.TEXT),\n\u001b[32m     24\u001b[39m         Property(name=\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m, data_type=DataType.TEXT),\n\u001b[32m     25\u001b[39m     ],\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     vectorizer_config=\u001b[43mConfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext2vec_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers/multi-qa-MiniLM-L6-cos-v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvectorize_collection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Load and insert data\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📂 Loading BBC data...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: _Vectorizer.text2vec_transformers() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "# SETUP CELL - Run this if the collection doesn't exist\n",
    "# This cell creates the bbc_collection and loads data if it doesn't already exist\n",
    "\n",
    "try:\n",
    "    # Try to get the collection first\n",
    "    collection_exists = client.collections.exists(\"bbc_collection\")\n",
    "    if not collection_exists:\n",
    "        print(\"🔧 Collection doesn't exist. Creating and loading data...\")\n",
    "        \n",
    "        # Import required modules for collection creation\n",
    "        from weaviate.classes.config import Configure, Property, DataType\n",
    "        import joblib\n",
    "        \n",
    "        # Create collection with schema\n",
    "        collection = client.collections.create(\n",
    "            name=\"bbc_collection\",\n",
    "            properties=[\n",
    "                Property(name=\"article_content\", data_type=DataType.TEXT),\n",
    "                Property(name=\"chunk\", data_type=DataType.TEXT),\n",
    "                Property(name=\"chunk_index\", data_type=DataType.INT),\n",
    "                Property(name=\"description\", data_type=DataType.TEXT),\n",
    "                Property(name=\"link\", data_type=DataType.TEXT),\n",
    "                Property(name=\"pubDate\", data_type=DataType.TEXT),\n",
    "                Property(name=\"title\", data_type=DataType.TEXT),\n",
    "            ],\n",
    "            vectorizer_config=Configure.Vectorizer.text2vec_transformers(\n",
    "                model=\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\",\n",
    "                vectorize_collection_name=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Load and insert data\n",
    "        print(\"📂 Loading BBC data...\")\n",
    "        bbc_data = joblib.load('data/bbc_data.joblib')\n",
    "        print(f\"📊 Found {len(bbc_data)} data items\")\n",
    "        \n",
    "        print(\"🔄 Inserting data into Weaviate...\")\n",
    "        with collection.batch.dynamic() as batch:\n",
    "            for i, item in enumerate(bbc_data):\n",
    "                data_object = {\n",
    "                    \"article_content\": item.get(\"article_content\", \"\"),\n",
    "                    \"chunk\": item.get(\"chunk\", \"\"),\n",
    "                    \"chunk_index\": item.get(\"chunk_index\", 0),\n",
    "                    \"description\": item.get(\"description\", \"\"),\n",
    "                    \"link\": item.get(\"link\", \"\"),\n",
    "                    \"pubDate\": item.get(\"pubDate\", \"\"),\n",
    "                    \"title\": item.get(\"title\", \"\"),\n",
    "                }\n",
    "                batch.add_object(properties=data_object)\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"  📤 Inserted {i + 1}/{len(bbc_data)} objects...\")\n",
    "        \n",
    "        print(f\"✅ Successfully created collection and loaded {len(bbc_data)} objects!\")\n",
    "    else:\n",
    "        print(\"✅ Collection 'bbc_collection' already exists!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during setup: {e}\")\n",
    "    print(\"You may need to run: python setup_collection.py\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Loading the data\n",
    "\n",
    "Now, let's load the data. The dataset is structured with the following fields:\n",
    "\n",
    "- **`title`**: The headline of the article.\n",
    "- **`pubDate`**: The publication date and time of the article.\n",
    "- **`guid`**: A unique identifier for the article, commonly used for listing.\n",
    "- **`link`**: A URL link to access the full article online.\n",
    "- **`description`**: A brief summary or teaser of the article's content.\n",
    "- **`article_content`**: The complete text of the article, providing detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbc_data = joblib.load('data/bbc_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_content: Justin Welby speaks on BBC Radio 4's Today programme as part of a special show guest edited by Dame ...(truncated)\n",
      "description: The Archbishop of Canterbury urges politicians to \"forswear wedge issues\" and avoid divisive topics.\n",
      "guid: https://www.bbc.co.uk/news/uk-67844356\n",
      "link: https://www.bbc.co.uk/news/uk-67844356?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-01 00:00:04\n",
      "title: Justin Welby: Political leaders should treat opponents as human beings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_object_properties(bbc_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Loading the Collection\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you will load the collection containing the BBC News dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection = client.collections.get(\"bbc_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of elements in the collection is: 383\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of elements in the collection is: {len(collection)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fetch one example of object in this collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the properties (some will be truncated due to size)\n",
      "article_content: Sir Keir Starmer flew in a private jet paid for by Qatar to visit the country's leader last month, a...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: The Labour leader made the return trip whilst attending the COP28 climate conference held in Dubai.\n",
      "link: https://www.bbc.co.uk/news/uk-politics-67952479?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-11T22:34:24.000000+00:00\n",
      "title: Keir Starmer used Qatar-funded private jet for Emir trip\n",
      "\n",
      "Available vector keys: dict_keys(['default'])\n",
      "Vector key: default\n",
      "Vector: (truncated) [0.329537034034729, 0.18794147670269012, 0.07612818479537964, 0.0717492550611496, 0.10402892529964447, 0.1416739970445633, 0.27558499574661255, -0.1952795535326004, -0.2213171124458313, 0.0204525887966156, 0.023724311962723732, 0.00579287251457572, -0.0062513998709619045, 0.025987591594457626, 0.07012374699115753]\n",
      "Vector length:  384\n"
     ]
    }
   ],
   "source": [
    "object = collection.query.fetch_objects(limit=1, include_vector=True).objects[0]\n",
    "print(\"Printing the properties (some will be truncated due to size)\")\n",
    "print_object_properties(object.properties)\n",
    "\n",
    "# Debug: Check what keys are in the vector\n",
    "print(\"Available vector keys:\", object.vector.keys() if object.vector else \"No vector data\")\n",
    "\n",
    "# Try to access the vector data with the correct key\n",
    "if object.vector:\n",
    "    # Get the first available vector key\n",
    "    vector_key = list(object.vector.keys())[0]\n",
    "    print(f\"Vector key: {vector_key}\")\n",
    "    print(f\"Vector: (truncated)\", object.vector[vector_key][0:15])\n",
    "    print(f\"Vector length: \", len(object.vector[vector_key]))\n",
    "else:\n",
    "    print(\"No vector data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the properties (some will be truncated due to size)\n",
      "article_content: Sir Keir Starmer flew in a private jet paid for by Qatar to visit the country's leader last month, a...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: The Labour leader made the return trip whilst attending the COP28 climate conference held in Dubai.\n",
      "link: https://www.bbc.co.uk/news/uk-politics-67952479?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-11T22:34:24.000000+00:00\n",
      "title: Keir Starmer used Qatar-funded private jet for Emir trip\n",
      "\n",
      "Vector: (truncated) [0.329537034034729, 0.18794147670269012, 0.07612818479537964, 0.0717492550611496, 0.10402892529964447, 0.1416739970445633, 0.27558499574661255, -0.1952795535326004, -0.2213171124458313, 0.0204525887966156, 0.023724311962723732, 0.00579287251457572, -0.0062513998709619045, 0.025987591594457626, 0.07012374699115753]\n",
      "Vector length:  384\n"
     ]
    }
   ],
   "source": [
    "object = collection.query.fetch_objects(limit = 1, include_vector = True).objects[0]\n",
    "print(\"Printing the properties (some will be truncated due to size)\")\n",
    "print_object_properties(object.properties)\n",
    "print(\"Vector: (truncated)\",object.vector['default'][0:15])\n",
    "print(\"Vector length: \", len(object.vector['default']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector length is `768`. So every chunk in the vector database is mapped into a 768 dimension vector. This is the vector the Weaviate API uses to perform semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='ex01'></a>\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 Metadata filtering\n",
    "\n",
    "<a id='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "In this exercise, you will implement a metadata filtering function. This function will take several inputs: a property (such as `article_content`, `title`, `pubDate`, etc.), the values you want to filter by, the collection you want to search in, and the number of items you want to retrieve.\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint 1</summary>\n",
    "<p>Remember that to perform filtering based only on metadata, the appropriate method to use is <code>collection.query.fetch_objects</code>.</p>\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint 2</summary>\n",
    "<p>When using <code>collection.query.fetch_objects</code>, you must provide the <code>metadata_property</code> as the <code>property</code> and the corresponding <code>Filter</code> object.</p>\n",
    "</details>\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint 3</summary>\n",
    "<p>The filter object should be used with the method <code>.by_property</code> for the appropriate property, and <code>.contains_any</code> with the relevant values. A typical call would be <code>Filter.by_property(metadata_property).contains_any(values)</code>.</p>\n",
    "<p>To limit the results, use <code>limit=limit</code> within the <code>.fetch_objects</code> method.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def filter_by_metadata(metadata_property: str, \n",
    "                       values: list[str], \n",
    "                       collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                       limit: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves objects from a specified collection based on metadata filtering criteria.\n",
    "\n",
    "    This function queries a collection within the specified client to fetch objects that match \n",
    "    certain metadata criteria. It uses a filter to find objects whose specified 'property' contains \n",
    "    any of the given 'values'. The number of objects retrieved is limited by the 'limit' parameter.\n",
    "\n",
    "    Args:\n",
    "    metadata_property (str): The name of the metadata property to filter on.\n",
    "    values (List[str]): A list of values to be matched against the specified property.\n",
    "    collection_name (weaviate.collections.collection.sync.Collection): The collection to query.\n",
    "    limit (int, optional): The maximum number of objects to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    List[Object]: A list of objects from the collection that match the filtering criteria.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve using collection.query.fetch_objects\n",
    "    response = collection.query.fetch_objects(\n",
    "        filters=Filter.by_property(metadata_property).contains_any(values),\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_content: The 2024 awards season kicked off in style at the Golden Globes - the first major red carpet event o...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: Stars including Margot Robbie and Taylor Swift arrived in a variety of eye-catching outfits.\n",
      "link: https://www.bbc.co.uk/news/entertainment-arts-67908727?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T03:23:58.000000+00:00\n",
      "title: Margot Robbie, Taylor Swift and more on Golden Globes red carpet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's get an example\n",
    "res = filter_by_metadata('title', ['Taylor Swift'], collection, limit = 2)\n",
    "for x in res:\n",
    "    print_object_properties(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "article_content: The 2024 awards season kicked off in style at the Golden Globes - the first major red carpet event o...(truncated)\n",
    "chunk: some of his previous get-ups. The Bear's Jeremy Allen White - who recently became the new face (and ...(truncated)\n",
    "chunk_index: 4\n",
    "description: Stars including Margot Robbie and Taylor Swift arrived in a variety of eye-catching outfits.\n",
    "link: https://www.bbc.co.uk/news/entertainment-arts-67908727?at_medium=RSS&at_campaign=KARANGA\n",
    "pubDate: 2024-01-08 03:23:58+00:00\n",
    "title: Margot Robbie, Taylor Swift and more on Golden Globes red carpet\n",
    "\n",
    "article_content: The 2024 awards season kicked off in style at the Golden Globes - the first major red carpet event o...(truncated)\n",
    "chunk: headpiece - not entirely a fashion choice. She says the \"protective veil\" is because she hurt her fa...(truncated)\n",
    "chunk_index: 5\n",
    "description: Stars including Margot Robbie and Taylor Swift arrived in a variety of eye-catching outfits.\n",
    "link: https://www.bbc.co.uk/news/entertainment-arts-67908727?at_medium=RSS&at_campaign=KARANGA\n",
    "pubDate: 2024-01-08 03:23:58+00:00\n",
    "title: Margot Robbie, Taylor Swift and more on Golden Globes red carpet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mFailed test case: Incorrect number of elements for function call filter_by_metadata(property = chunk, values = ['Brazil'], limit = 2).\n",
      "Expected: 2\n",
      "Got: 0\n",
      "\n",
      "\u001b[91mFailed test case: Incorrect number of elements for function call filter_by_metadata(property = chunk, values = ['France'], limit = 2).\n",
      "Expected: 2\n",
      "Got: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your solution!\n",
    "unittests.test_filter_by_metadata(filter_by_metadata, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex02'></a>\n",
    "\n",
    "<a id='3-2'></a>\n",
    "### 3.2 Semantic search\n",
    "\n",
    "<a id='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "In this exercise, you will implement a semantic search retrieval, similar to the one you created in the previous assignment, but this time utilizing the Weaviate API.\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint</summary>\n",
    "<p>Remember that to perform semantic search, you should use the method <code>collection.query.near_text</code>.</p>\n",
    "<p>The <code>top_k</code> parameter in the function dictates how many results to retrieve. In Weaviate, this is referred to as <code>limit</code>. Adjust this parameter as needed.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def semantic_search_retrieve(query: str,\n",
    "                             collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                             top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Performs a semantic search on a collection and retrieves the top relevant chunks.\n",
    "\n",
    "    This function executes a semantic search query on a specified collection to find text chunks \n",
    "    that are most relevant to the input 'query'. The search retrieves a limited number of top \n",
    "    matching objects, as specified by 'top_k'. The function returns the 'chunk' property of \n",
    "    each of the top matching objects.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query used to find relevant text chunks.\n",
    "    collection (weaviate.collections.collection.sync.Collection): The collection in which the semantic search is performed.\n",
    "    top_k (int, optional): The number of top relevant objects to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks that are most relevant to the given query.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Retrieve using collection.query.near_text\n",
    "    response = collection.query.near_text(\n",
    "    query=query,\n",
    "    limit=top_k\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_content: The Golden Globe Awards on Sunday had everything - jokes which fell flat, public displays of affecti...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: One commentator joked his monologue was \"a bomb\" and viewers also commented on his one-liners.\n",
      "link: https://www.bbc.co.uk/news/entertainment-arts-67908840?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T06:24:43.000000+00:00\n",
      "title: Host Jo Koy's jokes fall flat and six other Golden Globes moments\n",
      "\n",
      "article_content: It will be Georgia Harrison's first reality TV appearance since ex Stephen Bear was jailed for posti...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: ITV's new series will see the return of islanders like Georgia Harrison, Kaz Kamwi and Anton Danyluk.\n",
      "link: https://www.bbc.co.uk/news/newsbeat-67911425?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T11:55:31.000000+00:00\n",
      "title: Love Island All Stars: ITV reveals line-up for 2024 series\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's have an example!\n",
    "print_object_properties(semantic_search_retrieve(query = 'Tell me about the last Taylor Swift show', collection = collection, top_k = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "article_content: Taylor Swift has finished the European leg of her Eras Tour with a record-breaking show at Wembley S...(truncated)\n",
    "chunk: size crowd at all\". At an earlier show in Liverpool, she had also called the Eras Tour the “most exh...(truncated)\n",
    "chunk_index: 10\n",
    "description: The star is joined by Florence + The Machine and sings So Long, London at her final UK show.\n",
    "link: https://www.bbc.com/news/articles/cr5nr3n6epvo\n",
    "pubDate: 2024-08-21 03:02:08+00:00\n",
    "title: 'I've never had it this good' - Taylor Swift thanks fans after new Wembley record\n",
    "\n",
    "article_content: Taylor Swift has finished the European leg of her Eras Tour with a record-breaking show at Wembley S...(truncated)\n",
    "chunk: regular part of the setlist. Last week, the star was joined by Ed Sheeran to play the songs Endgame ...(truncated)\n",
    "chunk_index: 4\n",
    "description: The star is joined by Florence + The Machine and sings So Long, London at her final UK show.\n",
    "link: https://www.bbc.com/news/articles/cr5nr3n6epvo\n",
    "pubDate: 2024-08-21 03:02:08+00:00\n",
    "title: 'I've never had it this good' - Taylor Swift thanks fans after new Wembley record\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mFailed test case: title value of 0-th object returned is different from After France's election shock comes the real power struggle.\n",
      "Expected: semantic_search_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to After France's election shock comes the real power struggle\n",
      "Got: returned title is French booze-free January falls flat with Macron government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 0-th object returned is different from After France's election shock comes the real power struggle.\n",
      "Expected: semantic_search_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to After France's election shock comes the real power struggle\n",
      "Got: returned title is French booze-free January falls flat with Macron government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 0-th object returned is different from After France's election shock comes the real power struggle.\n",
      "Expected: semantic_search_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to After France's election shock comes the real power struggle\n",
      "Got: returned title is French booze-free January falls flat with Macron government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 0-th object returned is different from After France's election shock comes the real power struggle.\n",
      "Expected: semantic_search_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to After France's election shock comes the real power struggle\n",
      "Got: returned title is French booze-free January falls flat with Macron government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 1-th object returned is different from Sandi Toksvig officiates wedding of Abba's Björn Ulvaeus.\n",
      "Expected: semantic_search_retrieve(query = 'Famous actor marries', top_k = 1) must return 'title' equals to Sandi Toksvig officiates wedding of Abba's Björn Ulvaeus\n",
      "Got: returned title is Mary Poppins actress Glynis Johns dies aged 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unittests.test_semantic_search_retrieve(semantic_search_retrieve, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex03'></a>\n",
    "\n",
    "<a id='3-3'></a>\n",
    "### 3.3 BM25 Serach\n",
    "\n",
    "<a id='ex03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "In this exercise, you will implement a BM25 retrieval, similar to the one you created in the previous assignment, but now using the Weaviate API.\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint</summary>\n",
    "<p>To perform a BM25 search, use the method <code>collection.query.bm25</code>.</p>\n",
    "<p>The <code>top_k</code> parameter in the function specifies how many results to retrieve. In Weaviate, this parameter is referred to as <code>limit</code>. Adjust this accordingly.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def bm25_retrieve(query: str, \n",
    "                  collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                  top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Performs a BM25 search on a collection and retrieves the top relevant chunks.\n",
    "\n",
    "    This function executes a BM25-based search query on a specified collection to identify text \n",
    "    chunks that are most relevant to the provided 'query'. It retrieves a limited number of the \n",
    "    top matching objects, as specified by 'top_k', and returns the 'chunk' property of these objects.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query used to find relevant text chunks.\n",
    "    collection (weaviate.collections.collection.sync.Collection): The collection in which the BM25 search is performed.\n",
    "    top_k (int, optional): The number of top relevant objects to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks that are most relevant to the given query.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Retrieve using collection.query.bm25\n",
    "    response = collection.query.bm25(\n",
    "    query=query,\n",
    "    limit=top_k\n",
    ")\n",
    "\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    return response_objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_content: The Golden Globe Awards on Sunday had everything - jokes which fell flat, public displays of affecti...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: One commentator joked his monologue was \"a bomb\" and viewers also commented on his one-liners.\n",
      "link: https://www.bbc.co.uk/news/entertainment-arts-67908840?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T06:24:43.000000+00:00\n",
      "title: Host Jo Koy's jokes fall flat and six other Golden Globes moments\n",
      "\n",
      "article_content: The 2024 awards season kicked off in style at the Golden Globes - the first major red carpet event o...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: Stars including Margot Robbie and Taylor Swift arrived in a variety of eye-catching outfits.\n",
      "link: https://www.bbc.co.uk/news/entertainment-arts-67908727?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T03:23:58.000000+00:00\n",
      "title: Margot Robbie, Taylor Swift and more on Golden Globes red carpet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_object_properties(bm25_retrieve('Tell me about the last Taylor Swift show', collection, top_k = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "article_content: Rapper Killer Mike won three Grammys in the rap category - best rap song, best rap performance and b...(truncated)\n",
    "chunk: police brutality and systemic racism. He was a highly visible supporter of Bernie Sanders' two campa...(truncated)\n",
    "chunk_index: 4\n",
    "description: The 48-year-old was detained on a misdemeanour charge after winning three awards in the rap category.\n",
    "link: https://www.bbc.co.uk/news/world-us-canada-68201021?at_medium=RSS&at_campaign=KARANGA\n",
    "pubDate: 2024-02-05 23:27:08+00:00\n",
    "title: Killer Mike dismisses arrest at Grammys as 'speed bump'\n",
    "\n",
    "article_content: Rapper Killer Mike won three Grammys in the rap category - best rap song, best rap performance and b...(truncated)\n",
    "chunk: Nicki Minaj. He also won a third award for best rap album with his album Michael. \"You cannot tell m...(truncated)\n",
    "chunk_index: 3\n",
    "description: The 48-year-old was detained on a misdemeanour charge after winning three awards in the rap category.\n",
    "link: https://www.bbc.co.uk/news/world-us-canada-68201021?at_medium=RSS&at_campaign=KARANGA\n",
    "pubDate: 2024-02-05 23:27:08+00:00\n",
    "title: Killer Mike dismisses arrest at Grammys as 'speed bump'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mFailed test case: title value of 0-th object returned is different from D-Day remembrance planes will be found, says Shapps.\n",
      "Expected: bm25_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to D-Day remembrance planes will be found, says Shapps\n",
      "Got: returned title is Gabriel Attal: Youngest French PM hopes to revive Macron's government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 0-th object returned is different from D-Day remembrance planes will be found, says Shapps.\n",
      "Expected: bm25_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to D-Day remembrance planes will be found, says Shapps\n",
      "Got: returned title is Gabriel Attal: Youngest French PM hopes to revive Macron's government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 0-th object returned is different from D-Day remembrance planes will be found, says Shapps.\n",
      "Expected: bm25_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to D-Day remembrance planes will be found, says Shapps\n",
      "Got: returned title is Gabriel Attal: Youngest French PM hopes to revive Macron's government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 0-th object returned is different from D-Day remembrance planes will be found, says Shapps.\n",
      "Expected: bm25_retrieve(query = 'Conflicts in France', top_k = 2) must return 'title' equals to D-Day remembrance planes will be found, says Shapps\n",
      "Got: returned title is Gabriel Attal: Youngest French PM hopes to revive Macron's government\n",
      "\n",
      "\u001b[91mFailed test case: title value of 1-th object returned is different from Media tycoon Rupert Murdoch marries for fifth time.\n",
      "Expected: bm25_retrieve(query = 'Famous actor marries', top_k = 1) must return 'title' equals to Media tycoon Rupert Murdoch marries for fifth time\n",
      "Got: returned title is Sag Awards: Oppenheimer leads nominations as Leonardo DiCaprio snubbed\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unittests.test_bm25_retrieve(bm25_retrieve, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex04'></a>\n",
    "\n",
    "<a id='3-4'></a>\n",
    "### 3.4 Hybrid search\n",
    "\n",
    "<a id='ex04'></a>\n",
    "### Exercise 4\n",
    "\n",
    "In this exercise, you will implement a Reciprocal Rank Fusion (RRF) retrieval system using the Weaviate API. To achieve this, you will need to use the `collection.query.hybrid` method.\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint</summary>\n",
    "<p>To perform a hybrid search, use the method <code>collection.query.hybrid</code>.</p>\n",
    "<p>The <code>top_k</code> parameter in the function specifies how many results to retrieve. In Weaviate, this is referred to as <code>limit</code>. Make sure to also include the <code>alpha</code>.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def hybrid_retrieve(query: str, \n",
    "                    collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                    alpha: float = 0.5,\n",
    "                    top_k: int = 5\n",
    "                   ) -> list:\n",
    "    \"\"\"\n",
    "    Performs a hybrid search on a collection and retrieves the top relevant chunks.\n",
    "\n",
    "    This function executes a hybrid search that combines semantic vector search and traditional \n",
    "    keyword-based search on a specified collection to find text chunks most relevant to the \n",
    "    input 'query'. The relevance of results is influenced by 'alpha', which balances the weight \n",
    "    between vector and keyword matches. It retrieves a limited number of top matching objects, \n",
    "    as specified by 'top_k', and returns the 'chunk' property of these objects.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query used to find relevant text chunks.\n",
    "    collection (weaviate.collections.collection.sync.Collection): The collection in which the hybrid search is performed.\n",
    "    alpha (float, optional): A weighting factor that balances the contribution of semantic \n",
    "    and keyword matches. Defaults to 0.5.\n",
    "    top_k (int, optional): The number of top relevant objects to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks that are most relevant to the given query.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    # Retrieve using collection.query.hybrid\n",
    "    response = collection.query.hybrid(\n",
    "    query=query,\n",
    "    alpha=alpha,\n",
    "    limit=top_k\n",
    ")\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_content: The Golden Globe Awards on Sunday had everything - jokes which fell flat, public displays of affecti...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: One commentator joked his monologue was \"a bomb\" and viewers also commented on his one-liners.\n",
      "link: https://www.bbc.co.uk/news/entertainment-arts-67908840?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T06:24:43.000000+00:00\n",
      "title: Host Jo Koy's jokes fall flat and six other Golden Globes moments\n",
      "\n",
      "article_content: The 2024 awards season kicked off in style at the Golden Globes - the first major red carpet event o...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: Stars including Margot Robbie and Taylor Swift arrived in a variety of eye-catching outfits.\n",
      "link: https://www.bbc.co.uk/news/entertainment-arts-67908727?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T03:23:58.000000+00:00\n",
      "title: Margot Robbie, Taylor Swift and more on Golden Globes red carpet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_object_properties(hybrid_retrieve('Tell me about the last Taylor Swift show', collection, top_k = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "article_content: Rapper Killer Mike won three Grammys in the rap category - best rap song, best rap performance and b...(truncated)\n",
    "chunk: police brutality and systemic racism. He was a highly visible supporter of Bernie Sanders' two campa...(truncated)\n",
    "chunk_index: 4\n",
    "description: The 48-year-old was detained on a misdemeanour charge after winning three awards in the rap category.\n",
    "link: https://www.bbc.co.uk/news/world-us-canada-68201021?at_medium=RSS&at_campaign=KARANGA\n",
    "pubDate: 2024-02-05 23:27:08+00:00\n",
    "title: Killer Mike dismisses arrest at Grammys as 'speed bump'\n",
    "\n",
    "article_content: Taylor Swift has finished the European leg of her Eras Tour with a record-breaking show at Wembley S...(truncated)\n",
    "chunk: size crowd at all\". At an earlier show in Liverpool, she had also called the Eras Tour the “most exh...(truncated)\n",
    "chunk_index: 10\n",
    "description: The star is joined by Florence + The Machine and sings So Long, London at her final UK show.\n",
    "link: https://www.bbc.com/news/articles/cr5nr3n6epvo\n",
    "pubDate: 2024-08-21 03:02:08+00:00\n",
    "title: 'I've never had it this good' - Taylor Swift thanks fans after new Wembley record\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "unittests.test_hybrid_retrieve(hybrid_retrieve, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Reranking\n",
    "\n",
    "<a id='ex05'></a>\n",
    "\n",
    "<a id='ex05'></a>\n",
    "### Exercise 5\n",
    "\n",
    "In this section, you will create a new version of `semantic_search` that allows reranking of the results. This new function must support using a different query for reranking or reranking based on a specific document property (e.g., reranking using only the title).\n",
    "\n",
    "Your task is to add the `rerank` parameter to the `collection.query.near_text` call.\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint 1</summary>\n",
    "<p>Remember that <code>collection.query.near_text</code> takes a query, a limit (i.e., <code>top_k</code>), and now also requires the <code>rerank</code> parameter.</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint 2</summary>\n",
    "<p>The <code>Rerank</code> object is already loaded into memory. It takes two parameters: the query and the document property to use for ranking—<code>query</code> and <code>prop</code>, respectively.</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color: green;\">Hint 3</summary>\n",
    "<p>Define the reranker as <code>reranker = Reranker(appropriate_parameters)</code>. Don’t forget: the query for the reranker should be <code>rerank_query</code>!</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def semantic_search_with_reranking(query: str, \n",
    "                                   rerank_property: str,\n",
    "                                   collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                                   rerank_query: str = None,\n",
    "                                   top_k: int = 5\n",
    "                                   ) -> list:\n",
    "    \"\"\"\n",
    "    Performs a semantic search and reranks the results based on a specified property.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to perform the initial search.\n",
    "        rerank_property (str): The property used for reranking the search results.\n",
    "        collection (weaviate.collections.collection.sync.Collection): The collection to search within.\n",
    "        rerank_query (str, optional): The query to use specifically for reranking. If not provided, \n",
    "                                      the original query is used for reranking.\n",
    "        top_k (int, optional): The maximum number of top results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of properties from the reranked search results, where each item corresponds to \n",
    "              an object in the collection.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    # Set the rerank_query to be the same as the query if rerank_query is not passed (don't change this line)\n",
    "    if rerank_query is None: \n",
    "        rerank_query = query \n",
    "        \n",
    "    # Define the reranker with rerank_query and rerank_property\n",
    "    reranker = Rerank(\n",
    "    query=rerank_query,\n",
    "    prop=rerank_property\n",
    ")\n",
    "\n",
    "    # Retrieve using collection.query.near_text with the appropriate parameters (do not forget the rerank!)\n",
    "    response = collection.query.near_text(\n",
    "    query=query,\n",
    "    limit=top_k,\n",
    "    rerank=reranker\n",
    ")\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reranker model receives a query and a passage (in our case, a chunk of the result) to compute a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a query\n",
    "query = 'Tell me about the conflicts in Latin America'\n",
    "# Get the results from a search (in this case the hybrid search)\n",
    "results = semantic_search_with_reranking(query, collection = collection, top_k = 2, rerank_property = 'chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_content: Only two states have chosen which Republican they want to take on Democratic President Joe Biden but...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: The first votes are cast on Monday in an election being watched not just in the US but around the world.\n",
      "link: https://www.bbc.co.uk/news/world-us-canada-67949560?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-13T00:52:24.000000+00:00\n",
      "title: US election 2024: Why the world is watching so closely\n",
      "\n",
      "article_content: Polls show Taiwanese faith in the US is dwindling The rumour was old, but effective: the Taiwanese w...(truncated)\n",
      "chunk: ...(truncated)\n",
      "chunk_index: 0\n",
      "description: Beijing is pushing \"US scepticism\" to drive a wedge between Taiwan and its biggest ally, say analysts.\n",
      "link: https://www.bbc.co.uk/news/world-asia-67891869?at_medium=RSS&at_campaign=KARANGA\n",
      "pubDate: 2024-01-08T01:17:37.000000+00:00\n",
      "title: The dark rumours trying to destroy Taiwan's trust in the US\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_object_properties(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results**\n",
    "```\n",
    "article_content: A huge diplomatic row has erupted after Spain's transport minister suggested Argentina's president h...(truncated)\n",
    "chunk: weeks' to attend the launch of Vox's European election campaign, newspaper El Pais reported. Mr Mile...(truncated)\n",
    "chunk_index: 3\n",
    "description: A row breaks out after Spain's transport minister suggests Argentina's president has taken drugs.\n",
    "link: https://www.bbc.com/news/articles/czd8qzvpl4lo\n",
    "pubDate: 2024-05-04 15:56:45+00:00\n",
    "title: Spain-Argentina row over drug-use accusation\n",
    "\n",
    "article_content: Opposition supporters have gathered across Venezuela to protest against Nicolás Maduro's disputed vi...(truncated)\n",
    "chunk: the world, from Australia to Spain and also in the United Kingdom, Canada, Colombia, Mexico and Arge...(truncated)\n",
    "chunk_index: 4\n",
    "description: Opposition leader María Corina Machado joined thousands of demonstrators in the capital Caracas.\n",
    "link: https://www.bbc.com/news/articles/cgedgqqy7x9o\n",
    "pubDate: 2024-08-17 23:19:48+00:00\n",
    "title: Protests across Venezuela as election dispute goes on\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mFailed test case: Incorrect output for semantic_search_with_reranking(query = This is a test query, collection = collection,rerank_query = This is a test rerank query, top_k = 5, rerank_property = chunk).\n",
      "Expected: [\"The Papers: Israel's 'tragic error' and Labour's 'pro-building' bid\", 'MoT boss says 72-day wait for test is new normal', 'Pour a proper pint, Trading Standards tells pubs', 'Our interactive guide to the latest voting trends', 'Tories need a Budget bounce but can Hunt deliver?']\n",
      "Got: ['Quiz of the week: Why is Emilia Clarke going to the palace?', 'Quiz of the week: How did Lily Gladstone say thank you for her Golden Globe?', \"Give us credible offer and we'll end strikes - junior doctors\", \"Junior doctors' strike: Some hospitals request staff return to work\", 'The key NHS targets that have never been met']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "unittests.test_semantic_search_with_reranking(semantic_search_with_reranking, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Incorporating the Weaviate API into our previous schema\n",
    "---\n",
    "\n",
    "This section is not graded. Here, you will revisit the functions used throughout the assignments to integrate the Weaviate API into your existing schema. Once integrated, you will be able to run prompts and test your new RAG system!\n",
    "\n",
    "<a id='4-1'></a>\n",
    "### 4.1 Generating the final prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_final_prompt(query: str, \n",
    "                          top_k: int, \n",
    "                          retrieve_function: callable,\n",
    "                          rerank_query: str = None, \n",
    "                          rerank_property: str = None, \n",
    "                          use_rerank: bool = False, \n",
    "                          use_rag: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generates a final prompt by optionally retrieving and formatting relevant documents using retrieval-augmented generation (RAG).\n",
    "\n",
    "    Args:\n",
    "        query (str): The initial query to be used for document retrieval.\n",
    "        top_k (int): The number of top documents to retrieve and use for generating the prompt.\n",
    "        retrieve_function (callable): The function used to retrieve documents based on the query.\n",
    "        rerank_query (str, optional): The query used specifically for reranking documents if reranking is enabled.\n",
    "        rerank_property (str, optional): The property used for reranking. Required if 'use_rerank' is True.\n",
    "        use_rerank (bool, optional): Flag to denote whether to use reranking in document retrieval. Defaults to False.\n",
    "        use_rag (bool, optional): Flag to determine whether to use retrieval-augmented generation. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: A constructed prompt that includes the original query and formatted retrieved documents if 'use_rag' is True.\n",
    "             Otherwise, it returns the original query.\n",
    "    \"\"\"\n",
    "    # If no rag, return the query\n",
    "    if not use_rag:\n",
    "        return query\n",
    "    \n",
    "    if use_rerank:\n",
    "        if rerank_property is None:\n",
    "            raise ValueError('rerank_property must be set if use_rerank = True')\n",
    "        top_k_documents = retrieve_function(query=query, top_k=top_k, collection = collection, rerank_property = rerank_property, rerank_query = rerank_query)\n",
    "    else:\n",
    "        top_k_documents = retrieve_function(query=query, top_k=top_k, collection = collection)\n",
    "    \n",
    "    # Initialize an empty string to store the formatted data.\n",
    "    formatted_data = \"\"\n",
    "    \n",
    "    # Iterate over each retrieved document.\n",
    "    for document in top_k_documents:\n",
    "        # Format each document into a structured string.\n",
    "        document_layout = (\n",
    "            f\"Title: {document['title']}, Chunk: {document['chunk']}, \"\n",
    "            f\"Published at: {document['pubDate']}\\nURL: {document['link']}\"\n",
    "        )\n",
    "        # Append the formatted string to the main data string with a newline for separation.\n",
    "        formatted_data += document_layout + \"\\n\"\n",
    "    \n",
    "    # If use_rag flag is True, construct the enhanced prompt with the augmented data.\n",
    "    retrieve_data_formatted = formatted_data  # Store formatted data.\n",
    "    prompt = (\n",
    "        f\"Answer the user query below. There will be provided additional information for you to compose your answer. \"\n",
    "        f\"The relevant information provided is from 2024 and it should be added as your overall knowledge to answer the query, \"\n",
    "        f\"you should not rely only on this information to answer the query, but add it to your overall knowledge.\"\n",
    "        f\"The news data is ordered by relevance.\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        f\"2024 News: {retrieve_data_formatted}\"\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = generate_final_prompt(\"Tell me the economic situation of the US in 2024.\", top_k = 5, retrieve_function = semantic_search_retrieve, use_rerank = False, rerank_property = 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user query below. There will be provided additional information for you to compose your answer. The relevant information provided is from 2024 and it should be added as your overall knowledge to answer the query, you should not rely only on this information to answer the query, but add it to your overall knowledge.The news data is ordered by relevance.Query: Tell me the economic situation of the US in 2024.\n",
      "2024 News: Title: UK economic growth rebounds in November, Chunk: , Published at: 2024-01-12T10:15:50.000000+00:00\n",
      "URL: https://www.bbc.co.uk/news/business-67943106?at_medium=RSS&at_campaign=KARANGA\n",
      "Title: BBC Verify: Would Labour spend £28bn a year on green projects?, Chunk: , Published at: 2024-01-04T16:55:09.000000+00:00\n",
      "URL: https://www.bbc.co.uk/news/67801144?at_medium=RSS&at_campaign=KARANGA\n",
      "Title: Oil prices rise on US-UK strikes over Red Sea attacks, Chunk: , Published at: 2024-01-12T16:33:57.000000+00:00\n",
      "URL: https://www.bbc.co.uk/news/business-67947795?at_medium=RSS&at_campaign=KARANGA\n",
      "Title: US election 2024: Why the world is watching so closely, Chunk: , Published at: 2024-01-13T00:52:24.000000+00:00\n",
      "URL: https://www.bbc.co.uk/news/world-us-canada-67949560?at_medium=RSS&at_campaign=KARANGA\n",
      "Title: Household energy price rise of 5% comes into force, Chunk: , Published at: 2024-01-01T00:00:16.000000+00:00\n",
      "URL: https://www.bbc.co.uk/news/business-67785266?at_medium=RSS&at_campaign=KARANGA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### 4.2 LLM call\n",
    "\n",
    "Let's revisit the `llm_call` function, now adapted to this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_call(query: str, \n",
    "             retrieve_function: callable = None, \n",
    "             top_k: int = 5, \n",
    "             use_rag: bool = True, \n",
    "             use_rerank: bool = False, \n",
    "             rerank_property: str = None, \n",
    "             rerank_query: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Simulates a call to a language model by generating a prompt and using it to produce a response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The initial query for which a response is sought.\n",
    "        retrieve_function (callable, optional): The function used to retrieve documents related to the query.\n",
    "        top_k (int, optional): The number of top documents to retrieve and use for generating the prompt. Defaults to 5.\n",
    "        use_rag (bool, optional): Indicates whether to use retrieval-augmented generation. Defaults to True.\n",
    "        use_rerank (bool, optional): Indicates whether to apply reranking to the retrieved documents. Defaults to False.\n",
    "        rerank_property (str, optional): The property to use for reranking. Required if 'use_rerank' is True.\n",
    "        rerank_query (str, optional): The query used specifically for reranking documents if reranking is enabled.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response content after processing the prompt with a language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the prompt\n",
    "    PROMPT = generate_final_prompt(query, top_k = top_k, retrieve_function = retrieve_function, use_rag = use_rag, use_rerank = use_rerank, rerank_property = rerank_property, rerank_query = rerank_query)\n",
    "    \n",
    "    generated_response = generate_with_single_input(PROMPT)\n",
    "\n",
    "    generated_message = generated_response['content']\n",
    "    \n",
    "    return generated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Tell me about United States and Brazil's relationship over the course of 2024. Provide links for the resources you use in the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\xampp9\\htdocs\\building-rag-systems-with-a-vector-database\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.dlai.link'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error while calling LLM: f<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n<TITLE>ERROR: The request could not be satisfied</TITLE>\n</HEAD><BODY>\n<H1>403 ERROR</H1>\n<H2>The request could not be satisfied.</H2>\n<HR noshade size=\"1px\">\nRequest blocked.\nWe can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n<BR clear=\"all\">\nIf you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n<BR clear=\"all\">\n<HR noshade size=\"1px\">\n<PRE>\nGenerated by cloudfront (CloudFront)\nRequest ID: shwu2yfyAPjOjl6_TZQN00nRzS_UaOu2a7bdTrHPwZ8TNrUz_87Zbg==\n</PRE>\n<ADDRESS>\n</ADDRESS>\n</BODY></HTML>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Result with reranked results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m               \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m               \u001b[49m\u001b[43mretrieve_function\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhybrid_retrieve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mllm_call\u001b[39m\u001b[34m(query, retrieve_function, top_k, use_rag, use_rerank, rerank_property, rerank_query)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Get the prompt\u001b[39;00m\n\u001b[32m     25\u001b[39m PROMPT = generate_final_prompt(query, top_k = top_k, retrieve_function = retrieve_function, use_rag = use_rag, use_rerank = use_rerank, rerank_property = rerank_property, rerank_query = rerank_query)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m generated_response = \u001b[43mgenerate_with_single_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m generated_message = generated_response[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m generated_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\xampp9\\htdocs\\building-rag-systems-with-a-vector-database\\utils.py:96\u001b[39m, in \u001b[36mgenerate_with_single_input\u001b[39m\u001b[34m(prompt, role, top_p, temperature, max_tokens, model, together_api_key, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m response = requests.post(url, json = payload, verify=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.ok:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while calling LLM: f\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     98\u001b[39m     json_dict = json.loads(response.text)\n",
      "\u001b[31mException\u001b[39m: Error while calling LLM: f<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n<TITLE>ERROR: The request could not be satisfied</TITLE>\n</HEAD><BODY>\n<H1>403 ERROR</H1>\n<H2>The request could not be satisfied.</H2>\n<HR noshade size=\"1px\">\nRequest blocked.\nWe can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n<BR clear=\"all\">\nIf you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n<BR clear=\"all\">\n<HR noshade size=\"1px\">\n<PRE>\nGenerated by cloudfront (CloudFront)\nRequest ID: shwu2yfyAPjOjl6_TZQN00nRzS_UaOu2a7bdTrHPwZ8TNrUz_87Zbg==\n</PRE>\n<ADDRESS>\n</ADDRESS>\n</BODY></HTML>"
     ]
    }
   ],
   "source": [
    "# Result with reranked results\n",
    "print(llm_call(query = query, \n",
    "               top_k = 5, \n",
    "               retrieve_function = hybrid_retrieve, \n",
    "               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Experimenting with Your RAG System\n",
    "\n",
    "Now it is time for you to experiment with the system! Run the next cell to load a widget that will input a query, a rerank property, and output five different LLM responses:\n",
    "\n",
    "1. With semantic search\n",
    "2. With semantic search and reranking\n",
    "3. With BM25 search\n",
    "4. With hybrid search\n",
    "5. Without RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\xampp9\\htdocs\\building-rag-systems-with-a-vector-database\\venv\\Lib\\site-packages\\traitlets\\traitlets.py:1385: DeprecationWarning: Passing unrecognized arguments to super(Layout).__init__(align='center').\n",
      "object.__init__() takes exactly one argument (the instance to initialize)\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  warn(\n",
      "c:\\xampp9\\htdocs\\building-rag-systems-with-a-vector-database\\venv\\Lib\\site-packages\\traitlets\\traitlets.py:1385: DeprecationWarning: Passing unrecognized arguments to super(ButtonStyle).__init__(font_color='black').\n",
      "object.__init__() takes exactly one argument (the instance to initialize)\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40e817699e94164bc5ae465ecd4ea5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n    <style>\\n        .custom-output {\\n            background-color: #f9f9f9;\\n            color…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf7f96761a2483fba2f9809057a7066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Query:', layout=Layout(width='10%')), Text(value=\"Tell me about United States and …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d2fc12b6d54c91ad98be1db972f84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5, description='Top K:', max=20, min=1, style=SliderStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2474fcfb39894488947622ae0dab1f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Rerank Property:', options=('title', 'chunk'), style=DescriptionStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ee3231bfd74db49eb5c04b6eede4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Get Responses', style=ButtonStyle(button_color='#eee'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34324607367f486db3de3f1474fa0b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf204eedb92b4a1991174e7456deefff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Label(value='Semantic Search'), Output(layout=Layout(border_bottom='1px solid #c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16f9a42034242d080bf74e5c0889d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Label(value='Hybrid Search'), Output(layout=Layout(border_bottom='1px solid #ccc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "Error while calling LLM: f<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n<TITLE>ERROR: The request could not be satisfied</TITLE>\n</HEAD><BODY>\n<H1>403 ERROR</H1>\n<H2>The request could not be satisfied.</H2>\n<HR noshade size=\"1px\">\nRequest blocked.\nWe can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n<BR clear=\"all\">\nIf you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n<BR clear=\"all\">\n<HR noshade size=\"1px\">\n<PRE>\nGenerated by cloudfront (CloudFront)\nRequest ID: aHGouaTJh85-gVrKk-Znw8i0sf1FH9WKIX812qa3E8FG6kI_PAwt8w==\n</PRE>\n<ADDRESS>\n</ADDRESS>\n</BODY></HTML>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\xampp9\\htdocs\\building-rag-systems-with-a-vector-database\\utils.py:190\u001b[39m, in \u001b[36mdisplay_widget.<locals>.on_button_click\u001b[39m\u001b[34m(b)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    189\u001b[39m     rr_property = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m response = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rag\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_rag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieve_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rerank\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_rerank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrerank_property\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrr_property\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m output:\n\u001b[32m    192\u001b[39m     display(Markdown(response))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mllm_call\u001b[39m\u001b[34m(query, retrieve_function, top_k, use_rag, use_rerank, rerank_property, rerank_query)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Get the prompt\u001b[39;00m\n\u001b[32m     25\u001b[39m PROMPT = generate_final_prompt(query, top_k = top_k, retrieve_function = retrieve_function, use_rag = use_rag, use_rerank = use_rerank, rerank_property = rerank_property, rerank_query = rerank_query)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m generated_response = \u001b[43mgenerate_with_single_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m generated_message = generated_response[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m generated_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\xampp9\\htdocs\\building-rag-systems-with-a-vector-database\\utils.py:96\u001b[39m, in \u001b[36mgenerate_with_single_input\u001b[39m\u001b[34m(prompt, role, top_p, temperature, max_tokens, model, together_api_key, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m response = requests.post(url, json = payload, verify=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.ok:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while calling LLM: f\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     98\u001b[39m     json_dict = json.loads(response.text)\n",
      "\u001b[31mException\u001b[39m: Error while calling LLM: f<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n<TITLE>ERROR: The request could not be satisfied</TITLE>\n</HEAD><BODY>\n<H1>403 ERROR</H1>\n<H2>The request could not be satisfied.</H2>\n<HR noshade size=\"1px\">\nRequest blocked.\nWe can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n<BR clear=\"all\">\nIf you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n<BR clear=\"all\">\n<HR noshade size=\"1px\">\n<PRE>\nGenerated by cloudfront (CloudFront)\nRequest ID: aHGouaTJh85-gVrKk-Znw8i0sf1FH9WKIX812qa3E8FG6kI_PAwt8w==\n</PRE>\n<ADDRESS>\n</ADDRESS>\n</BODY></HTML>"
     ]
    }
   ],
   "source": [
    "display_widget(llm_call, semantic_search_retrieve, bm25_retrieve, hybrid_retrieve, semantic_search_with_reranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing this assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
